<html class="pc pagepc" style="font-size: 91.3889px;">
<head>
<meta charset="UTF-8">
<title>腾讯 AI Lab - 腾讯人工智能实验室官网</title>
<meta name="renderer" content="webkit">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta name="baidu-site-verification" content="r5znRc9TGh">
<meta name="viewport"
	content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
<meta
	content="腾讯AI,QQ AI,腾讯人工智能实验室,，腾讯人工智能，计算机视觉，语音合成，语音识别，语音技术，语音识别与合成，自然语言处理，机器学习，大数据，深度学习，视频滤镜，图像滤镜，图像描述生成，语音识别前端，神经网络，信息挖掘，信息抽取，对话，问答，机器翻译"
	name="Keywords">
<meta name="description"
	content="腾讯 AI Lab（腾讯人工智能实验室）， 腾讯公司级AI战略，聚集全球数十位人工智能科学家、50位世界一流AI博士。聚焦Content AI, Social AI，Game AI. 
专注机器学习、计算机视觉、语音识别、自然语言处理等人工智能领域的研究。基于腾讯亿万用户海量数据及在互联网各垂直领域的技术优势，立志打造世界顶尖人工智能团队。
">
<meta name="format-detection" content="telephone=no,email=no">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta name="author" content="Tencent-CDC">
<meta name="copyright" content="Tencent">
<meta name="robots" content="index,follow">
<link href="/favicon.ico" type="image/ico" rel="shortcut icon">
<link rel="stylesheet" type="text/css" href="/css/idangerous.swiper.css">
<link rel="stylesheet" type="text/css" href="/css/style.css">

<script src="//pingjs.qq.com/h5/stats.js?v2.0.2" name="MTAH5"
	sid="500355814" cid="500357202"></script>
<script>
(function(doc, win) {
    var docEl = doc.documentElement,
        resizeEvt = 'orientationchange' in window ? 'orientationchange' : 'resize',
        recalc = function () {
            var designWidth  = 1920; // 设计稿宽度
            var designHeight = 1080; // 设计稿高度
            var maxWidth     = 1920; // 设计稿最大宽度
            var maxHeight    = 1080; // 设计稿最大高度
            var clientWidth  = docEl.clientWidth;
            var clientHeight = docEl.clientHeight;
            var baseFontSize = 100; // 根元素基准字体大小
            if ( (navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i))) {
                //移动端
                docEl.setAttribute("class", "m"); 
                baseFontSize = 256; // 根元素基准字体大小
                // console.log('m');
                $('.header').addClass('fixed');
            } else {
                //PC端
                docEl.setAttribute("class", "pc pagepc");
                baseFontSize = 100; // 根元素基准字体大小
                // console.log('pc');
            }

            if (!clientWidth) return;
            if (clientWidth / clientHeight > designWidth / designHeight) {
                if (clientHeight <= maxHeight) {
                    docEl.style.fontSize = clientHeight / designHeight * baseFontSize + "px";
                } else {
                    docEl.style.fontSize = maxHeight / designHeight * baseFontSize + "px";
                }
            } else {
                if (clientWidth <= maxWidth) {
                    docEl.style.fontSize = clientWidth / designWidth * baseFontSize + "px";
                } else {
                    docEl.style.fontSize = maxWidth / designWidth * baseFontSize + "px";
                }
            }
        };

    // Abort if browser does not support addEventListener
    if (!doc.addEventListener) return;
    win.addEventListener(resizeEvt, recalc, false);
    doc.addEventListener('DOMContentLoaded', recalc, false);
})(document, window);

$(document).ready(function(){
    loadProperties();
});

</script>
<script type="text/javascript" src="/js/sona.min.js"></script>
<style type="text/css">
#sonaOrientation {
	position: fixed;
	top: 0;
	left: 0;
	height: 100%;
	width: 100%;
	background: #131313;
	z-index: 999
}

#sonaOrientation.hide {
	display: none
}

#sonaOrientation.sona-transparent {
	background-color: rgba(0, 0, 0, .85)
}

#sonaOrientation.sona-close .hinter_close {
	display: block
}

#sonaOrientation .hinter_close {
	position: absolute;
	top: 20px;
	right: 20px;
	height: 24px;
	width: 24px;
	background:
		url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAMAAABg3Am1AAAAflBMVEUAAAD////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////vroaSAAAAKXRSTlMAw8kPCxnzy/wFciP2ixQSlY9c6L63YE9DFmpXOe7p29nQzXhsKZtmO6BiKe8AAAHoSURBVEjHpZbZcuIwEEXbJJIl72CzmC0BEpL7/z84FVWGLuvKw8P0AxRd5xTudmsRivGwXL+fqur0vl4eRnkSbZFjEnnRynyY2gIurzffpTHl96bOHVB1Zgb3hQXyj3byjx85YAuf4ssrkO04v8uAyyfntxb9SpKx6mG3cbIGBjNb2wD3NU3tgfM/2uHvwH7yPA49Fi+zwmsGuK3+/rToXhZQg/nsDvuo3F8weAnGLP/qB1z+drfA2YiokeJFzBk3CWEsQj/VYD50F9aETIdMhAziRTJ0P19NhZ2wwbzsUDWhglyEDeZFchThcy9sMB9eby4ywjVCRpKXxsHI4cciI+L1mVayRC1pg3mpsZQ1NpI0lNfYYC1vOErKMMprHPEmFqWkDAvmpYQVoBEOY2ENp1sgErSfNlROQhU9kvbfpCaxxCkuWvsfesVFU1u1/8GgtuqLi3k2wovT0SCejTAaI1zLPBk6fDrexKuh460LiHk2chS6RIknIyxR3QSIZyNDp9sM8WwcHuNVoDfEk2F63H4T/orBRzwZ7YCrf8yUxT3iyTjDlpPtPvDzRg9sowOl8/O87xCP3Jd7dmT976H4/Njl8LcKWMQH+wKobnPlmeXv1eFYjmN55KsDR8OXk4ag5PXHuZnrzx+IlDamiY+WTQAAAABJRU5ErkJggg==)
		no-repeat 50% transparent;
	background-size: 24px 24px;
	overflow: hidden;
	text-indent: 100%;
	white-space: nowrap;
	opacity: .8;
	display: none
}

#sonaOrientation .hinter_phone {
	position: absolute;
	top: 50%;
	left: 50%;
	margin: -30px 0 0 -57px
}

#sonaOrientation .hinter_phone i {
	display: block;
	width: 114px;
	height: 60px;
	background:
		url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAOQAAAB4CAMAAADVLa6HAAABCFBMVEUAAAAAAAAAAABWVlYAAADFxcUAAACKioovLy8AAAAAAAAAAAC+vr6+vr6EhIQAAAALCwsBAQEGBgbIyMjGxsYAAADGxsbHx8cAAADHx8fAwMCoqKi9vb1SUlIGBgbHx8fFxcW0tLSxsbGgoKC7u7slJSVeXl6ysrLFxcW/v7+7u7uoqKiRkZEUFBQFBQWMjIxxcXHExMTExMTCwsK6urq2trbBwcHExMSsrKzExMS1tbWUlJTDw8O0tLSnp6dpaWm9vb1ZWVmDg4PAwMCvr6+Ojo4+Pj4xMTFKSkpnZ2e2trahoaGxsbGfn5+3t7ecnJyvr6+CgoKVlZU9PT1WVlasrKy8vLzJycmXdzvDAAAAV3RSTlMAARMjBL8WfEggDinGSGgeBwsj++IU8vkb9N6oeE0x7tBMt49nLykPxtLEmoM9NxcG6dvl0Ma6sLCnnYmHb11cW1hSUUFAOzo2NCQcqpmOfXx1bUY9Ny21pUe+AAAC6klEQVR42uzaV2/bMBSGYVI9kRs6aanhWc/YiffKHs1oM5t0r+///5MyrgvXqR0XuZIP+N5IF7rQAxIHAkQxTpJWDo+URySmRPri48oghcUvKDbef/UT3j9Mqaub4FSydePqSSb9KAGZtU9v3208W/g2Lk8OQ8Ns+4r+NlZTSFbWXaWJSC58BuGdlIC1dYfGxhywc+V6JPhEOhdgZ6yUZ8CruMOJOGSWM1iLqxHrNmOMiptRCFkOcOTr3/e72C6wW8ehLIfkN3cou0DqTZqjUYh+yexRT5j2sB/3pGDZKjKdBAlRC8B1IYWQm4gtG9w5sgVn2kKS5yxAcwbmazNgvfvLvrlMaQmL0JWSj+/XbEEJcYCKrxcX+dx9FHmLVNcRYhftWcj2ddS/ZOcia0AnIY0ltjwDGfO1jHZzkcI8MULSDORy1KfufyFdi7TISGSRFmmR0coiLdIio9UTkMMfPh5Jzkiqhkg1vyeIM/IcK63DTCOuJGPkXlhIe8c4dYkxcmklrsQqXry0yEhlkRZpkdHKIi3SIqOVRVqkRUYri+SJJK2U4zhKE1sk9Y6bYRAMmpUb1yOeyH6uiFGp1rpDkh9S3tWBbMWc17383DB3eVdLdsi7EMmY72gy6XKIYt4lbsh+HVv59J+BI3slZK8TxAyZQzKfpvGc7YX3JwclK2StiCPfmMaVEeRdYoU8w1Y3MfGuVMcHX7NCHgyPtU5UxXZBsULW8eXhq/7EoOuwQoY4fXhkt4agwwv5q707OGEYiKEg2kUgpyQtpBn334nB+OgChN78DgYWe1eg+Z8nyGMZ5PfpuL6WHVfiw0P8QojLAHGtMy7oxFPLeDQT4497kPV/7x5kGSNJZLgcZJCjE2SQQc5KkEEGOStButsExF4IseFD7GoZW3dBjkyQQQY5K0EGGeSsBClCrle6EXI+QrNICDMJ9SkhsV2tI/5dOmJFLE0owgnZu6HtJwoYjCoNoxTFqLcxiop2V06dXT9t25ovKx4AAAAASUVORK5CYII=)
		no-repeat 50% transparent;
	background-size: 114px 60px;
	-webkit-animation: rotateHint 2.6s ease-in infinite;
	animation: rotateHint 2.6s ease-in infinite
}

#sonaOrientation.sona-portrait .hinter_phone {
	-webkit-transform: rotate(90deg);
	transform: rotate(90deg)
}

#sonaOrientation .hinter_rotate {
	width: 155px;
	height: 30px;
	position: absolute;
	top: 50%;
	left: 50%;
	margin: -15px 0 0 -77px;
	background:
		url(data:image/png;base64,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)
		no-repeat 50% transparent;
	background-size: 155px 30px
}

#sonaOrientation .hinter_text {
	position: absolute;
	right: 0;
	bottom: 15%;
	left: 0;
	color: #bbb;
	font-size: 14px;
	font-weight: 400;
	text-align: center;
	line-height: 1
}

@
-webkit-keyframes rotateHint { 0%{
	-webkit-transform: rotate(0deg);
	opacity: .2
}

50%{
-webkit-transform
:rotate(90deg)
;opacity
:
1
}
to {
	-webkit-transform: rotate(90deg);
	opacity: 0
}

}
@
keyframes rotateHint { 0%{
	transform: rotate(0deg);
	opacity: .2
}

50%{
transform
:rotate(90deg)
;opacity
:
1
}
to {
	transform: rotate(90deg);
	opacity: 0
}
}
</style>
<script type="text/javascript" src="/js/jquery-2.0.0.min.js"></script>
<script type="text/javascript" src="/js/jquery.cookie.js"></script>
<script type="text/javascript" src="/js/jquery.i18n.properties.min.js"></script>
<script type="text/javascript" src="/js/main.js"></script>

<style>
#cVim-command-bar, #cVim-command-bar-mode, #cVim-command-bar-input,
	#cVim-command-bar-search-results, .cVim-completion-item,
	.cVim-completion-item .cVim-full, .cVim-completion-item .cVim-left,
	.cVim-completion-item .cVim-right {
	font-family: Helvetica, Helvetica Neue, Neue, sans-serif, monospace,
		Arial;
	font-size: 10pt !important;
	-webkit-font-smoothing: antialiased !important;
}

#cVim-command-bar {
	position: fixed;
	z-index: 2147483646;
	background-color: #1b1d1e;
	color: #bbb;
	display: none;
	box-sizing: content-box;
	box-shadow: 0 3px 3px rgba(0, 0, 0, 0.4);
	left: 0;
	width: 100%;
	height: 20px;
}

#cVim-command-bar-mode {
	display: inline-block;
	vertical-align: middle;
	box-sizing: border-box;
	padding-left: 2px;
	height: 100%;
	width: 10px;
	padding-top: 2px;
	color: #888;
}

#cVim-command-bar-input {
	background-color: #1b1d1e;
	color: #bbb;
	height: 100%;
	right: 0;
	top: 0;
	width: calc(100% - 10px);
	position: absolute;
}

#cVim-command-bar-search-results {
	position: fixed;
	width: 100%;
	overflow: hidden;
	z-index: 2147483647;
	left: 0;
	box-shadow: 0 3px 3px rgba(0, 0, 0, 0.4);
	background-color: #1c1c1c;
}

.cVim-completion-item, .cVim-completion-item .cVim-full,
	.cVim-completion-item .cVim-left, .cVim-completion-item .cVim-right {
	text-overflow: ellipsis;
	padding: 1px;
	display: inline-block;
	box-sizing: border-box;
	vertical-align: middle;
	overflow: hidden;
	white-space: nowrap;
}

.cVim-completion-item:nth-child(even) {
	background-color: #1f1f1f;
}

.cVim-completion-item {
	width: 100%;
	left: 0;
	color: #bcbcbc;
}

.cVim-completion-item[active] {
	width: 100%;
	left: 0;
	color: #1b1d1e;
	background-color: #f1f1f1;
}

.cVim-completion-item[active] span {
	color: #1b1d1e;
}

.cVim-completion-item .cVim-left {
	color: #fff;
	width: 37%;
}

.cVim-completion-item .cVim-right {
	font-style: italic;
	color: #888;
	width: 57%;
}

#cVim-link-container, .cVim-link-hint, #cVim-hud, #cVim-status-bar {
	font-family: Helvetica, Helvetica Neue, Neue, sans-serif, monospace,
		Arial;
	font-size: 10pt !important;
	-webkit-font-smoothing: antialiased !important;
}

#cVim-link-container {
	position: absolute;
	pointer-events: none;
	width: 100%;
	left: 0;
	height: 100%;
	top: 0;
	z-index: 2147483647;
}

.cVim-link-hint {
	position: absolute;
	color: #302505 !important;
	background-color: #ffd76e !important;
	border-radius: 2px !important;
	padding: 2px !important;
	font-size: 8pt !important;
	font-weight: 500 !important;
	text-transform: uppercase !important;
	border: 1px solid #ad810c;
	display: inline-block !important;
	vertical-align: middle !important;
	text-align: center !important;
	box-shadow: 2px 2px 1px rgba(0, 0, 0, 0.25) !important;
}

.cVim-link-hint_match {
	color: #777;
	text-transform: uppercase !important;
}

#cVim-hud {
	background-color: rgba(28, 28, 28, 0.9);
	position: fixed !important;
	transition: right 0.2s ease-out;
	z-index: 24724289;
}

#cVim-hud span {
	padding: 2px;
	padding-left: 4px;
	padding-right: 4px;
	color: #8f8f8f;
	font-size: 10pt;
}

#cVim-frames-outline {
	position: fixed;
	width: 100%;
	height: 100%;
	left: 0;
	top: 0;
	right: 0;
	z-index: 9999999999;
	box-sizing: border-box;
	border: 3px solid yellow;
}
</style>
</head>
<body style="" class="lang-cn">
	<div
		style="position: absolute; opacity: 0; z-index: -1; width: 0; height: 0; overflow: hidden;">
		<img id="mzsharethumbnail"
			src="https://ai.tencent.com/ailab/images/share.png"
			alt="Wechat Share Thumbnail">
	</div>
	<!--[if lt IE 10]><body id="lt-ie10"><![endif]-->




	<section class="g-wrap">
		<div th:replace="header::header('news')"></div>

		<div class="container">
			<div class="news-details-hd">
				<h3 class="news-title">CVPR 2018 | 腾讯AI Lab入选21篇论文详解</h3>
				<p class="news-athor">腾讯TEG AI Lab</p>
				<div class="news-date">2018年3月21日</div>
			</div>
			<div class="news-details-bd">
				<div class="paper-main news-content">
					<p
						style="margin-bottom: 0; text-align: justify; text-justify: inter-ideograph; line-height: 24px">
						<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">近十年来在国际计算机视觉领域最具影响力、研究内容最全面的顶级学术会议CVPR，近日揭晓2018年收录论文名单，腾讯AI
							Lab共有21篇论文入选，位居国内企业前列，我们将在下文进行详解，欢迎交流与讨论。</span>
					</p>
					<p
						style="margin-bottom: 0; text-align: justify; text-justify: inter-ideograph; line-height: 24px; max-width: 100%; box-sizing: border-box; word-wrap: break-word !important; min-height: 1em">
						<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">
							去年CVPR的论文录取率为29%，腾讯AI Lab 共有6篇论文入选，点击
							<a
								href="//mp.weixin.qq.com/s?__biz=MzIzOTg4MjEwNw==&amp;mid=2247483658&amp;idx=1&amp;sn=601ac676665ece84e1158c0cb0689343&amp;scene=21#wechat_redirect"
								target="_blank">
								<span
									style="font-family: 微软雅黑, sans-serif; font-size: 16px; color: black;">
									<span
										style="font-family: 微软雅黑, sans-serif; font-size: 16px; max-width: 100%; box-sizing: border-box; word-wrap: break-word !important;">这</span>
									里
								</span>
							</a>
							可以回顾。2017年，腾讯 AI
							Lab共有100多篇论文发表在AI顶级会议上，包括ICML（4篇）、ACL（3篇）、NIPS（8篇）等。
						</span>
					</p>
					<p
						style="margin-bottom: 0; text-align: justify; text-justify: inter-ideograph; line-height: 24px">
						<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">我们还坚持与学界、企业界和行业「共享AI
							未来」，已与美国麻省理工大学、英国牛津大学、香港科技大学、香港中文大学等多所海内外知名院校开展学术合作，并通过年度学术论坛、联合研究、访问学者、博士生及研究生奖学金等多种形式，推动前沿研究探索、应用与人才培养。</span>
					</p>
					<p style="margin-bottom: 0; text-align: center; line-height: 24px">
						<span style="font-size: 16px;">
							<strong>
								<span
									style="font-family: 微软雅黑, sans-serif; font-weight: normal;">计
									算 机 视 觉 未 来 方 向 与 挑 战</span>
							</strong>
							<strong>
								<span
									style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
							</strong>
						</span>
					</p>
					<p>
						<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">
							计算机视觉（Computer
							Vision）的未来，就是多媒体AI崛起，机器之眼被慢慢打开的未来。多媒体有的时候又称为富媒体，是对图像、语音、动画和交互信息的统称。多媒体AI就是对这些所有内容的智能处理。一份国际报告显示，到2021年，视频将占全球个人互联网流量的比例，将从15年的70%增长到82%，成为信息的主要载体。目前我们计算机视觉中心的工作重点，从以往单纯的图像转向视频AI，研究视频的编辑、理解、分析和生成等。
							<br>
						</span>
					</p>
					<section>
						<section>
							<section>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">第一个方向是研究如何让AI理解视频中更深层、更细节的信息，分析视频里人物与人物间、人物与物体间，到物体与场景间的具体关系，这是业界热门且亟待突破的研究方向。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">第二个方向，不仅要研究视觉信号，还着眼于多模态信息，如计算机视觉加文本、加语音等信号的结合。比如视觉
										文本上，我们的图像与视频描述生成技术已有一定</span>
									<a
										href="//mp.weixin.qq.com/s?__biz=MzIzOTg4MjEwNw==&amp;mid=2247483705&amp;idx=1&amp;sn=d6cf3cc060cd1a1945d10d6156015e3b&amp;scene=21#wechat_redirect"
										target="_blank"
										style="text-decoration: underline; font-family: 微软雅黑, sans-serif; font-size: 16px; color: rgb(0, 0, 0);">
										<span
											style="font-family: 微软雅黑, sans-serif; font-size: 16px; color: rgb(0, 0, 0);">进展</span>
									</a>
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">第三个方向是多媒体AI在垂直领域的应用。如在机器人领域，用视觉信息让AI感知周围世界，构建整个空间信息，进行导航和避障等操作。在医疗领域，分析医疗影像数据，结合病历文本信息等，让AI深入参与到辅助诊疗中。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">这个领域的未来挑战，更多是对具体应用场景，比如安防、无人驾驶等难度大的具体应用场景，进行更细致规划和技术延伸。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">腾
										讯 AI Lab 21 篇 入 选 论 文 详 解</span>
								</p>
								<p style="line-height: normal;">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">1.面向视频理解的端到端动作表示学习</span>
								</p>
								<p style="background: white; line-height: normal;">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">End-to-End
										Learning of Motion Representation for Video Understanding</span>
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; background: white; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文由AI
										Lab主导完成，并入选Spotlight文章。尽管端到端的特征学习已经取得了重要的进展，但是人工设计的光流特征仍然被广泛用于各类视频分析任务中。为了弥补这个不足，作者创造性地提出了一个能从数据中学习出类光流特征并且能进行端到端训练的神经网络：TVNet。当前，TV-L1方法通过优化方法来求解光流，是最常用的方法之一。作者发现，把TV-L1的每一步迭代通过特定设计翻译成神经网络的某一层，就能得到TVNet的初始版本。因此，TVNet能无需训练就能被直接使用。更重要的是，TVNet能被嫁接到任何分类神经网络来构建从数据端到任务端的统一结构，从而避免了传统多阶段方法中需要预计算、预存储光流的需要。最后，TVNet的某些参数是可以被通过端到端训练来进一步优化，这有助于TVNet学习出更丰富以及与任务更相关的特征而不仅仅是光流。在两个动作识别的标准数据集HMDB51和UCF101上，该方法取得了比同类方法更好的分类结果。与TV-L1相比，TVNet在节省光流提取时间和存储空间的基础上，明显提高了识别精度。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">2.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">基于递归的左右双目对比模型的立体匹配</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-size: 16px;">
										<em
											style="max-width: 100%; box-sizing: border-box; word-wrap: break-word !important">
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Left-Right
												Comparative Recurrent Model for Stereo Matching</span>
										</em>
										<em
											style="max-width: 100%; box-sizing: border-box; word-wrap: break-word !important">
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文由AI
										Lab主导完成。充分利用左右双目的视差信息对于立体视差估计问题非常关键。左右一致性检测是通过参考对侧信息来提高视差估计质量的有效方法。然而，传统的左右一致性检测是孤立的后处理过程，而且重度依赖手工设计。本文提出了一种全新的左右双目对比的递归模型，同时实现左右一致性检测和视差估计。在每个递归步上，模型同时为双目预测视差结果，然后进行在线左右双目对比并识别出很可能预测错误的左右不匹配区域。本文提出了一种“软注意力机制”更好地利用学习到的误差图来指导模型在下一步预测中有针对性地修正更新低置信度的区域。通过这种左右对比的递归模型，生成的视差图质量能够不断提高。在KITTI
										2015、Scene
										Flow和Middlebury标准库上的实验验证了本方法的有效性，并显示本方法能取得最高的立体匹配视差估计性能。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-size: 14px; font-family: 微软雅黑, sans-serif; color: rgb(136, 136, 136); font-weight: normal;">下载地址：</span>
										</strong>
										<span
											style="font-family: 微软雅黑, sans-serif; color: rgb(136, 136, 136); font-size: 16px; max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">https://arxiv.org/pdf/1801.09414.pdf</span>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-size: 14px; font-family: 微软雅黑, sans-serif; font-weight: normal;">3.MRF</span>
										</strong>
										<strong>
											<span
												style="font-size: 14px; font-family: 微软雅黑, sans-serif; font-weight: normal;">中的CNN：基于内嵌CNN的高阶时空MRF的视频对象分割</span>
										</strong>
										<strong>
											<span
												style="font-size: 14px; font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-size: 16px;">
										<em
											style="max-width: 100%; box-sizing: border-box; word-wrap: break-word !important">
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">CNN
												in MRF: Video Object Segmentation via Inference in A
												CNN-Based Higher-Order Spatio-Temporal MRF</span>
										</em>
										<em
											style="max-width: 100%; box-sizing: border-box; word-wrap: break-word !important">
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文由AI
										Lab独立完成。本文讨论了视频对象分割的问题，其中输入视频的第一帧初始对象的掩膜是给定的。作者提出了一个新的时空马尔可夫随机场（MRF）模型来解决问题。与传统的MRF模型不同，作者提出的模型中像素之间的空间相关性由卷积神经网络（CNN）编码。具体而言，对于给定的对象，可以通过用该对象预先训练的CNN来预测一组空间相邻像素进行分割标记的概率。因此，集合中像素之间的更高阶更丰富的依赖关系可以由CNN隐式建模。然后通过光流建立时间依赖关系，所得到的MRF模型结合了用于解决视频对象分割的空间和时间线索。然而，由于其中非常高阶的依赖关系，在MRF模型中执行推理非常困难。为此，作者提出了一种新颖的嵌入CNN的近似算法来有效地执行MRF模型中的推理。该算法通过迭代交替执行两个步骤：时间融合步骤和前馈CNN步骤。通过使用一种简单的基于外观的分割CNN进行初始化，作者提出的模型性能超过了DAVIS
										2017挑战赛的获奖方法，而无需借助模型集成或任何专用检测器。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">4.CosFace:
											</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">面向深度人脸识别的增强边缘余弦损失函数设计</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">CosFace:
												Large Margin Cosine Loss for Deep Face Recognition</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文由AI
										Lab独立完成。由于深度卷积神经网络(CNN)的研究进展，人脸识别已经取得了革命性的进展。人脸识别的核心任务包括人脸验证和人脸辨识。然而，在传统意义上的深度卷积神经网络的softmax代价函数的监督下，所学习的模型通常缺乏足够的判别性。为了解决这一问题，近期一系列损失函数被提出来，如Center
										Loss，L-Softmax，A-Softmax。所有这些改进算法都基于一个核心思想:增强类间差异并且减小类内差异。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">在本文中，作者从一个新的角度来解决这个问题，并设计了一个新的损失函数，即增强边缘余弦损失函数(LMCL)。更具体地说，通过对特征向量和权向量的L2归一化，把softmax损失函数转化为余弦损失函数，这样做消除了半径方向的变化，并在此基础上引入了一个余弦边界值m来进一步最大化所学习的特征在角度空间的决策间距。因此，采用这种归一化和增强余弦决策边距的方法，能够更有效的起到最大化类间差异和最小化类内差异的作用。作者在最权威的人脸公开测试集上进行了实验评估，这些测试集包括MegaFace
										Challenge, Youtube Faces (YTF)，和Labeled Face in the Wild
										(LFW)，取得了极其优异的性能，验证了研发的新方法的有效性。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">5.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">类人化标注：多样性和独特性图像标注</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Tagging
												like Humans: Diverse and Distinct Image Annotation</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文由
										AI
										Lab主导完成。作者提出了一种全新的自动图像标注的生成式模型，名为多样性和独特性图像标注（D2IA）。受到人类标注集成的启发，D2IA将产生语义相关，独特且多样性的标签。第一步，利用基于行列式点过程（DPP）的序列采样，产生一个标签子集，使得子集中的每个标签与图像内容相关，且标签之间语义上是独特的（即没有语义冗余）。第二步，对DPP模型加上随机扰动得到不同的概率分布，进而可以通过第一步中的序列采样产生多个不同的标签子集。作者利用生成对抗网络（GAN）来训练D2IA，在两个基准数据集上开展了充分的实验，包括定量和定性的对比，以及人类主观测试。实验结果说明，相对于目前最先进的自动图像标注方法，本文的方法可以产生更加多样和独特的标签。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">6.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">用当前重构过去的正则化RNN的描述生成</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Regularizing
												RNNs for Caption Generation by Reconstructing The Past with
												The Present</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文由
										AI
										Lab主导完成。近年来，基于编码-解码框架的描述生成技术被广泛的研究并应用于不同的领域，如图像的描述生成和代码的注释生成等。本文提出了一种新的名为自动重构网络（ARNet）的框架，这种框架可以与传统的编解码框架相结合，并以端到端的方式对输入数据生成描述。ARNet使用RNN中当前时刻的隐状态去重构前一个时刻的隐状态，基于此，ARNet可以鼓励当前时刻的隐状态去包含前一个时刻隐状态的更多信息，同时这样可以对RNN中的隐状态的变化起到正则化的作用。实验表明，本文所提出的ARNet在图像描述和代码注释任务上可以提高现今编解码网络的性能。另外，ARNet可以显著地缓解描述生成技术中训练过程与推断过程的不一致的问题。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">7.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">视频描述的重构网络</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Reconstruction
												Network for Video Captioning</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文由
										AI
										Lab主导完成。在论文中，利用自然语言描述视频序列的视觉内容使这个问题得到解决。不同于以前的视频生成描述工作，主要利用视频内容中的线索生成语言描述，本文提出一个重构网络（RecNet）和编码器-
										解码器- 重构器结构，该结构可同时利用前向信息流（从视频到语句）和后向信息流（从语句到视频）生成视频描述。具体来说，编码器
										-
										解码器利用前向信息流产生基于被编码视频语义特征的句子描述。作者设计了两种重构器来利用后向信息流，基于解码器的隐藏状态序列重构视频特征。由编码器
										-
										解码器得到的传统损失和由重构器造成的重构损失以端到端的形式联合训练RecNet。在基准数据集上的实验结果表明，所提出的重构器可以增强编码器
										- 解码器模型性能，并可显著提高视频描述的准确性。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">8.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">基于门限融合网络的图像去雾方法</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-size: 16px;">
										<em
											style="max-width: 100%; box-sizing: border-box; word-wrap: break-word !important">
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Gated
												Fusion Network for Single Image Dehazing</span>
										</em>
										<em
											style="max-width: 100%; box-sizing: border-box; word-wrap: break-word !important">
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文与中国科学院信息工程研究所、加州大学默赛德分校等合作完成。本文提出一种基于门限融合网络的雾霾图像的复原方法。该门限融合网络由一个编码解码网络构成。其中，编码网络用于对雾霾图像本身及其多种变换图像进行特征编码，解码网络则用于估计这些变换图像对应的权重。具体而言，对一张雾霾图像，作者对其进行多种变换，包括图像白平衡、对比度增强和伽马矫正等操作提取图像内部的不同颜色或对比度特征，然后将得到的变换图像输入到门限融合卷积神经网络中，利用神经网络对雾霾图像的每个变换图像估计一个权重矩阵，再利用权重矩阵对所有的变换图像进行融合获得最终的去雾结果图。另外，为了去除恢复结果中容易出现的光圈效应，作者提出了多尺度门限融合网络，可以有效增加网络感知野并减少光圈效应。在大量合成图片和真实图片上的实验证明作者提出的方法可以有效恢复雾霾图像的细节信息。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">9.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">基于双向注意融合机制和上下文门控的密集视频描述</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Bidirectional
												Attentive Fusion with Context Gating for Dense Video
												Captioning</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文由
										AI
										Lab主导完成。密集视频描述是一个时下刚兴起的课题，旨在同时定位并用自然语言描述一个长视频中发生的所有事件或行为。在这个任务中，本文明确并解决了两个挑战，即：（1）如何利用好过去和未来的信息以便更精确地定位出事件，（2）如何给解码器输入有效的视觉信息，以便更准确地生成针对该事件的自然语言描述。第一，过去的工作集中在从正向（视频从开头往结尾的方向）生成事件候选区间，而忽视了同样关键的未来信息。作者引入了一种双向提取事件候选区间的方法，同时利用了过去和未来的信息，从而更有效地进行事件定位。第二，过去的方法无法区分结束时间相近的事件，即给出的描述是相同的。为了解决这个问题，作者通过注意力机制将事件定位模块中的隐状态与视频原始内容（例如，视频C3D特征）结合起来表征当前的事件。进一步地，作者提出一种新颖的上下文门控机制来平衡当前事件内容和它的上下文对生成文字描述的贡献。作者通过大量的实验证明了所提出的注意力融合的事件表征方式相比于单独地使用隐状态或视频内容的表征方式要表现得更好。通过将事件定位模块和事件描述模块统一到一个框架中，本文的方法在ActivityNet
										Captions数据库上超过了之前最好的方法，相对性能提升100%（Meteor分数从4.82到9.65）。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">10.</span>
										</strong>
										<strong
											style="max-width: 100%; box-sizing: border-box; word-wrap: break-word !important">
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">基于多阶段生成对抗网的延时摄影视频的生成</span>
										</strong>
										<strong
											style="max-width: 100%; box-sizing: border-box; word-wrap: break-word !important">
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Learning
												to Generate Time-Lapse Videos Using Multi-Stage Dynamic
												Generative Adversarial Networks</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文由
										AI
										Lab主导完成。在户外拍摄一张照片之后，我们可以预测照片里面接下来发生的事情吗？比如，云彩会怎么移动？作者通过展示一个两阶段的生成对抗网来生成逼真的延时摄影视频对这个问题进行了回答。给定第一帧图像，本文的模型可以预测未来帧。在其两阶段模型里面，第一个阶段生成具有逼真内容的延时摄影视频。第二个阶段对第一个阶段的结果进行优化，主要体现在增加动态的运动信息，使之与真实的延时摄影视频更加接近。为了使最终生成的视频具有生动的运动信息，作者引入格拉姆矩阵来更加精确地描述运动信息。作者建立了一个大规模的延时摄影视频数据集，并且在这个数据集上面测试了其方法。通过使用该模型，可以生成分辨率为128x128，多达32帧的逼真的延时摄影视频。定性和定量实验都证明该方法相比已有最好模型的优越性。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-size: 14px; font-family: 微软雅黑, sans-serif; color: rgb(136, 136, 136); font-weight: normal;">下载地址</span>
										</strong>
										<span
											style="font-family: 微软雅黑, sans-serif; color: rgb(136, 136, 136); font-size: 16px; max-width: 100%; box-sizing: border-box !important; word-wrap: break-word !important;">：https://arxiv.org/abs/1709.07592</span>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">11.VITAL</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">：对抗式学习之视觉跟踪</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">VITAL:
												VIsual Tracking via Adversarial Learning</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文与澳大利亚阿德莱德大学、香港城市大学、加州大学默赛德分校等合作完成。现有的检测式跟踪的框架由两个阶段组成，即在目标物体周围大量采样和对每个样本进行正负分类。现有的基于深度学习结构的检测式跟踪的效果受限于两个问题。第一，每一帧中正样本高度重叠，他们无法捕获物体丰富的变化表征。第二，正负样本之间存在严重的不均衡分布的问题。本文提出VITAL这个算法来解决这两个问题。为了丰富正样本，作者采用生成式网络来随机生成遮罩。这些遮罩作用在输入特征上来捕获目标物体的一系列变化。在对抗学习的作用下，作者的网络能够识别出在整个时序中哪一种遮罩保留了目标物体的鲁邦性特征。与此同时，在解决正负样本不均衡的问题中，本文提出了一个高阶敏感损失来减小简单负样本对于分类器训练的影响。在标准数据库中大量的实验证明，本文提出的跟踪器优于目前已有的方法。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">12.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">再访空洞卷积:
												一种简单的弱监督和半监督语义分割方法</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Revisiting
												Dilated Convolution: A Simple Approach for Weakly- and Semi-
												Supervised Semantic Segmentation</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文与美国伊利诺伊大学香槟分校、新加坡国立大学合作完成。尽管弱监督语义分割已经取得了突出的进展，但相比于全监督的语义分割，弱监督语义分割效果依然不理想。作者观察到这其中的效果差距主要来自于仅仅依靠图像级别的标注，无法得到密集完整的像素级别的物体位置信息用来训练分割模型。本文重新探索空洞卷积并且阐明了它如何使分类网络生成密集的物体定位信息。通过依靠不同的倍率的空洞卷积来显著增大卷积核的感受野，分类网络能定位物体的非判别性区域，最终产生可靠的物体区域，有助于弱监督和半监督的语义分割。尽管该方法过程简单，但是能取得目前最高的语义分割性能。具体地说，该方法在弱监督语义分割和半监督语义分割的情况下，在Pascal
										VOC 2012测试集上能达到目前最高的60.8%和67.6% mIOU。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">13.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">使用语义保持对抗嵌入网络的zero-shot视觉识别</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Zero-Shot
												Visual Recognition using Semantics-Preserving Adversarial
												Embedding Networks</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文与南洋理工大学、哥伦比亚大学、浙江大学合作完成。本文提出了一种称为语义保持敌对嵌入网络（SP-AEN）的新型框架，用于zero-shot视觉识别（ZSL），其中测试图像及其类别在训练期间都是不可见的。
										SP-AEN旨在解决固有的问题 - 语义丢失 -
										在基于嵌入的ZSL的流行家族中，如果某些语义在训练期间不好被区分，则在训练期间会被丢弃，但是对测试样本是有意义的。具体而言，SP-AEN通过引入独立的视觉
										-
										语义空间嵌入来防止语义损失。该嵌入将语义空间分解为两个可争议相互矛盾的目标的两个子空间：分类和重建。通过对这两个子空间的对抗学习，SP-AEN可以将重构子空间的语义转移到可区分子空间，从而实现对未见类的zero-shot识别。与以前的方法相比，SP-AEN不仅可以改善分类效果，还可以生成照片般真实的图像，显示语义保存的有效性。在CUB，AWA，SUN和aPY上，SP-AEN的harmonic平均值分别为12.2％，9.3％，4.0％和3.6％，明显优于最先进的ZSL方法。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">14.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">用于跨模态检索的自监督对抗哈希网络</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Self-Supervised
												Adversarial Hashing Networks for Cross-Modal Retrieval</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文与西安电子科技大学、悉尼大学合作完成。由于深度学习的成功，最近跨模式检索获得了显着改进。但是，仍然存在一个关键的瓶颈，即如何缩小多模态之间的距离，进一步提高检索的准确性。本文提出了一种自我监督对抗哈希（SSAH）方法，这是早期试图将对抗性学习纳入以自我监督方式的跨模态哈希研究中。这项工作的主要贡献是作者采用了几个对抗网络来最大化不同模态之间的语义相关性和表示一致性。另外，作者利用自我监督的语义网络以多标签注释的形式发现高级语义信息，指导特征学习过程以保持共同语义空间和海明空间中的模态之间的关系。对三个基准数据集进行的大量实验表明，所提出的SSAH优于最先进的方法。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">15.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">左右非对称层跳跃网络</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Left/Right
												Asymmetric Layer Skippable Networks</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文与复旦大学合作完成。最近的神经科学研究表明，左右大脑在处理低空间频率和高空间频率的信息时是非对称的。受到这项研究的启发，作者提出了一种新的左右非对称层跳跃网络，用于由粗到精的物体分类。该网络包含两个分支来同时处理粗粒度与细粒度分类。同时，作者首次提出了层跳跃机制，它学习了一个门控网络来决定是否在测试阶段来略过某些层。层跳跃机制赋予了该网络更好的灵活性以及更大的容量。作者在多种常用数据库上进行了测试，结果表明该网络在处理由粗到精的物体分类问题上优于其他方法。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">16.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">利用空间变化循环神经网络对动态场景去模糊</span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Dynamic
												Scene Deblurring Using Spatially Variant Recurrent Neural
												Networks</span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文与南京理工大学、加州大学默塞德分校等合作完成。由于相机抖动，景深和物体运动，动态场景去模糊是空间变化的。已有的利用先验图像信息或者庞大的深度神经网络的方法无法有效地处理这个问题，同时他们计算开销很大。与已有的方法不同，作者提出一个空间变化的神经网络来解决动态场景去模糊。作者提出的算法包含了三个卷积神经网络和一个循环卷积神经网络。其中卷积神经网络用来提取特征，学习循环卷积神经网络的系数和图像重建。在卷积神经网络抽取的特征指引下，循环卷积神经网络恢复出清晰的图像。作者的分析表明，该算法具有较大的接受范围，同时模型尺寸较小。与此同时，本文分析了空间变化循环卷积网路和反卷积的关系。分析表明空间变化循环卷积网络能够对反卷积建模。作者以端到端训练的方式，提出一个较小的深度学习模型，其速度优于已有的方法。在标准数据库上定量和定性的评估表明该方法在精度，速度和模型大小方面优于已有的方法。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">17.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">利用深度往复式高动态范围转换进行图像校正</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Image
												Correction via Deep Reciprocating HDR Transformation</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文与大连理工大学、香港城市大学合作完成。图像校正的目的在于对一幅输入图像进行调整，使其视觉柔和的同时在过曝光和欠曝光的区域恢复出图像细节。然而，现有的图像校正的方法主要依据于图像像素操作，使得从过曝光或者欠曝光区域恢复出图像细节十分困难。为此，作者回顾了图像生成的过程，并注意到细节均保留在高动态区域中，人眼可感知。然而，在非线性成像生成低动态范围的过程中有部分细节丢失。基于此发现，作者将图像修复问题归为深度往复式高动态范围转换的过程，同时提出一个创新的方法——首先从高动态范围域中恢复出丢失的细节，然后将此细节转换到低动态范围的图像中作为输出结果。以这种方式，作者提出一个端到端的深度往复式高动态范围转换模型。该模型由两个卷积神经网络组成，第一个功能为高动态范围细节重建，另外一个为低动态范围细节校正。在标准数据库下的实验表明，相比于已有的图像校正方法，作者提出的方法更加有效。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">18.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">端到端的卷积语义嵌入</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">End-to-end
												Convolutional Semantic Embeddings</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文与美国罗切斯特大学合作完成。最近已经广泛研究了图像和句子的语义嵌入。深度神经网络在学习丰富而鲁棒的视觉和文本表示方面的能力为开发有效的语义嵌入模型提供了机会。目前，最先进的语义学习方法首先采用深度神经网络将图像和句子编码到一个共同的语义空间中。然后，学习目标是确保匹配图像和句子对之间的相似度比随机抽样对更大。通常，卷积神经网络（CNN）和递归神经网络（RNN）分别用于学习图像和句子表示。一方面，已知CNN在不同级别产生强健的视觉特征，并且RNN以捕获顺序数据中的依赖性而闻名。因此，这个简单的框架可以充分有效地学习视觉和文本语义。另一方面，与CNN不同，RNN不能产生中间级别（例如，文本中的短语级别）表示。因此，只有全局表示可用于语义学习。由于图像和句子中的层次结构，这可能会限制模型的性能。在这项工作中，作者应用卷积神经网络来处理图像和句子。因此，通过在卷积层上引入新的学习目标，作者可以采用中级表示来辅助全局语义学习。实验结果表明，本文提出的具有新学习目标的文本CNN模型导致比现有技术方法更好的性能。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">19.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">一种有效防止负迁移或灾难性遗忘的深度人脸检测的自适应算法</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Deep
												Face Detector Adaptation without Negative Transfer or
												Catastrophic Forgetting</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文与美国弗罗里达中央大学、Adobe
										Research合作完成。没有任何一个人脸检测器能够适用于所有场景，所以如何使检测器自适应不同场景从而提高在目标场景的准确率，就变得很有必要。作者提出一种新颖的针对深度人脸检测器的自适应算法。给定目标域的代表性的图像，无论它们是否被标记，该算法都能够有效的将检测器针对目标域进行优化。这个过程不需要存储任何源域的数据——原来用以训练检测器的数据。作者设计了一种残差目标函数来显式的避免在迁移学习中臭名昭著的负转移问题。与此同时，它不会对来自源域的知识造成灾难性的干扰或遗忘，使得自适应以后的人脸检测器不仅在目标域的准确率更高，
										并且在源域中与原始检测器保持大致相同的性能。从某种角度看，该方法和很流行的自然语言模型插值技术有一定相似，它有可能开创一个新的方向：如何从不同域的数据逐步训练好的人脸检测器。作者报告了广泛的实验结果，以验证在两种深度人脸检测器上的效果。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">20.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">基于几何指导下的卷积神经网络的视频表征自监督学习</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;"></span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Geometry-Guided
												CNN for Self-supervised Video Representation learning</span>
										</em>
										<em>
											<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;"></span>
										</em>
									</span>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文与清华大学、北京邮电大学、加州大学圣地亚哥分校、斯坦福大学合作完成。人工标注视频以训练高质量的视频识别模型通常费力且昂贵，因此，在学习视频表征的方向上，已经有一些兴趣和工作来探索不需要人工监督的、有噪声的、和间接的训练信号。然而，这些信号往往很粗糙——为整段视频提供监督，或者很微弱——比如强制机器解决对人来说都很难的识别问题。在本文中，作者改为探索不需人工标注的几何信息，这是辅助视频表征的自监督学习的一种全新的信号。作者将像素级几何信息从合成图像里提取为流场或者从3D电影里提取为视差图。虽然几何和高级的语义看似不相关，但令人惊讶的是，作者发现由这些几何线索训练的卷积神经网络可以被有效地应用于语义视频理解的任务。此外，作者还发现渐进式的训练策略——而不是盲目地将不同的几何线索源汇集在一起——可以为视频识别提供更好的神经网络。在视频动态场景识别和动作识别的结果表明，作者的几何指导下的卷积神经网络明显优于其他类型的自监督信号训练的竞争方法。</span>
								</p>
								<p style="line-height: 24px">
									<span style="font-size: 16px;">
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">21.</span>
										</strong>
										<strong>
											<span
												style="font-family: 微软雅黑, sans-serif; font-weight: normal;">学习在黑暗中看世界</span>
										</strong>
									</span>
								</p>
								<p style="line-height: 24px">
									<em>
										<span style="font-size: 14px; font-family: 微软雅黑, sans-serif;">Learning
											to See in the Dark</span>
									</em>
								</p>
								<p
									style="line-height: 24px; max-width: 100%; box-sizing: border-box; min-height: 1em; word-wrap: break-word !important">
									<span style="font-family: 微软雅黑, sans-serif; font-size: 16px;">本文与美国伊利诺伊大学香槟分校、Intel
										Labs合作完成。在低光照甚至黑暗条件下拍出高质量的照片一直是非常有挑战性的科研问题，这主要是由于低光子数和低信噪比给相机成像带来了很大的困难。曝光时间过短会给图像带来噪点，而长时间曝光又容易导致图像模糊，费时费力，在现实中可行性低。传统算法提出了各种去噪、去模糊和增强技术，但是它们的有效性在极端条件下是非常有限的，例如夜晚的视频成像。为了支持基于深度学习的低光图像处理流水线的开发，作者收集了一个大规模的夜间成像数据集，它由短曝光夜间图像以及相应的长曝光参考图像组成。使用这个数据集，作者开发了一个基于全卷积网络端到端训练的低光图像处理流水线。该网络直接读入原始传感器数据，然后前向输出一张高清图像。这个技术克服了传统图像处理流水线需要多模块且夜间成像效果差的不足。本文展示了新数据集颇具前景的结果，并分析了影响性能的因素，以及未来研究的机会。</span>
								</p>
							</section>
						</section>
					</section>
					<p>
						<br>
					</p>
					<section>
						<section>
							<section>
								<p style="line-height: 24px">
									<br>
								</p>
							</section>
						</section>
					</section>
					<p>
						<br>
					</p>
					<p>
						<br>
					</p>
					<section>
						<section>
							<section>
								<p
									style="margin-top: 0px; margin-bottom: 0px; padding: 0px; max-width: 100%; box-sizing: border-box; clear: both; min-height: 1em; white-space: normal; word-wrap: break-word !important;">
									<br>
								</p>
							</section>
						</section>
					</section>
					<section>
						<section>
							<section>
								<p
									style="margin-top: 0px; margin-bottom: 0px; padding: 0px; max-width: 100%; box-sizing: border-box; clear: both; min-height: 1em; white-space: normal; word-wrap: break-word !important;">
									<br>
								</p>
							</section>
						</section>
					</section>
					<p>
						<br>
					</p>
				</div>
				<div class="paper-side news-related">
					<div class="related-title">相关动态</div>
					<div class="news-related-item">
						<h4 class="news-title">
							<a href="news11.html">夺冠 | 腾讯围棋AI「绝艺」再次问鼎龙星战</a>
						</h4>
						<p class="news-desc">12月10日，腾讯AI
							Lab围棋AI“绝艺”在东京举办的2017围棋AI龙星战（AI
							RYUSEI）决赛夺冠，以平稳表现相继战胜DeepZenGo、MayoiGo、Raynz和AQ等国际领先的AI。</p>
						<span class="news-date">2017年12月10日</span>
					</div>

					<div class="news-related-item">
						<h4 class="news-title">
							<a href="news13.html">腾讯AI Lab发布三大核心战略，与自然科研达成战略合作</a>
						</h4>
						<p class="news-desc">3月15日，腾讯AI
							Lab第二届学术论坛在深圳举行，聚焦人工智能在医疗、游戏、多媒体内容、人机交互等四大领域的跨界研究与应用。全球30位顶级AI专家出席，对多项前沿研究成果进行了深入探讨与交流</p>
						<span class="news-date">2018年3月15日</span>
					</div>


				</div>
				<ul class="paper-num">
					<li class="paper-prev">
						<span class="prev">上一篇：</span>
						<div class="link">
							<a href="news11.html">夺冠 | 腾讯围棋AI「绝艺」再次问鼎龙星战</a>
						</div>
					</li>
					<li class="paper-next">
						<span class="next">下一篇：</span>
						<div class="link">
							<a href="news13.html">腾讯AI Lab发布三大核心战略，与自然科研达成战略合作</a>
						</div>
					</li>

				</ul>
			</div>
		</div>
		
		<div th:replace="footer::html"></div>

	</section>










</body>
<div id="cVim-status-bar" style="top: 0px;"></div>
<iframe
	src="chrome-extension://ihlenndgcmojhcghmfjfneahoeklbjjh/cmdline_frame.html"
	id="cVim-command-frame"></iframe>
</html>